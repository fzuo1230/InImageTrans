import os
import numpy as np
import seaborn
import numpy as np
import matplotlib as plt


with open("/home/notebook/code/personal/S9055503/Vary-main/Vary-master/explain/token_res/reco_token281.txt","r") as f:
    tokens = f.readlines()
f.close()


def softmax(lst):
    #print(lst)
    row_max = np.max(lst)
    lst = [x - row_max for x in lst]
    x_exp = [np.exp(x) for x in lst]
    x_sum = sum(x_exp)
    s = [x/x_sum for x in x_exp]
    return s

def get_attentions(lst):
    all_attentions= []
    x,y,z = 0,0,0
    count = 0
    num = len(tokens)/1024
    while z < num:
        layer_attentions = []
        while x<32:
            head_attention = []
            while y<32:
                thing = tokens[count].replace("[", "").replace("]", "").replace("\n", "")
                thing = thing.split(",")
                thing = [float(it) for it in thing]
                thing = np.array(thing,dtype=float)
                head_attention.append(thing)
                y+=1
                count+=1
            layer_attentions.append(head_attention)
            y=0
            x+=1
        all_attentions.append(layer_attentions)
        x=0
        z+=1
    return all_attentions

def aver_attn(lst):
    average_attn = []
    for it in lst:
        #token_attn = []
        layer_attn = []
        for layer in it:
            num = len(layer[0])
            sum_attn = [0 for index in range(num)]
            for head in layer:
                sum_attn = [x+y for x,y in zip(head,sum_attn)]
            sum_attn = [z/32 for z in sum_attn]

            #sum_attn = softmax(sum_attn)
            layer_attn.append(sum_attn)
        average_attn.append(layer_attn)
    return average_attn


def draw_heatmap(lst):
    plt.pyplot.figure(figsize=(200,20))
    x = list(range(1,len(lst[0])+1))
    y = list(range(1,33))
    ax =seaborn.heatmap(lst,xticklabels=x,yticklabels=y,annot=False)
    ax.set_title('attention')
    ax.set_xlabel('token')
    ax.set_ylabel('layer')
    figure = ax.get_figure()
    figure.savefig('./explain/single_token/reco_token281_attentions.jpg')


            
'''
f = [0.003147020936012268, 0.0025776848196983337, 0.004769902676343918, 0.003041796386241913, 0.005008440464735031, 0.004793662577867508, 0.011197730898857117, 0.0041917357593774796, 0.004988489672541618, 0.01610727608203888, 0.006569975987076759, 0.004907778464257717, 0.006712928414344788, 0.024613484740257263, 0.008263029158115387, 0.00996166467666626, 0.002301238477230072, 0.0038309842348098755, 0.004448451101779938, 0.005191076546907425, 0.00983593612909317, 0.0005732488352805376, 0.0006302769761532545, 0.0007041776552796364, 0.0005903318524360657, 0.0008459687232971191, 0.0011401139199733734, 0.0011815261095762253, 0.0006140377372503281, 0.00036480207927525043, 0.00036473385989665985, 0.0002412850153632462, 0.00022177211940288544, 0.0006409327033907175, 0.0008935779333114624, 0.00048030633479356766, 0.0006132954731583595, 0.0010622404515743256, 0.0006940187886357307, 0.0006174817681312561, 0.0005259457975625992, 0.0006497744470834732, 0.0003364142030477524, 0.0005525453016161919, 0.00030715949833393097, 0.00023420143406838179, 0.0002802090020850301, 0.0003273007459938526, 0.00032541342079639435, 0.0003499835729598999, 0.0004891231656074524, 0.00048242881894111633, 0.0005625728517770767, 0.000573284924030304, 0.0004734508693218231, 0.0001058485358953476, 0.0006565488874912262, 0.0008317455649375916, 0.000948198139667511, 0.0006794384680688381, 0.0004316512495279312, 0.0007936246693134308, 0.0008625686168670654, 0.0008833780884742737, 0.0004744362086057663, 0.00044892309233546257, 0.0002371643204241991, 0.00022742804139852524, 0.00040661729872226715, 0.00026828330010175705, 0.0006109848618507385, 0.0008061490952968597, 0.0007142471149563789, 0.0006115417927503586, 0.0004693102091550827, 0.0006719417870044708, 0.0006372164934873581, 0.0007269736379384995, 0.0005580908618867397, 0.00048238784074783325, 0.00033425772562623024, 0.0002475176006555557, 0.0005891025066375732, 0.0003366749733686447, 0.0005050711333751678, 0.00039935484528541565, 0.0006400793790817261, 0.0007286928594112396, 0.0007382519543170929, 0.0006851814687252045, 0.0008385423570871353, 0.0006476081907749176, 0.0007458105683326721, 0.0007372349500656128, 0.0006368793547153473, 0.0006785411387681961, 0.0003270551096647978, 0.0002926888410001993, 0.0003291366156190634, 0.00024628452956676483, 0.00025384314358234406, 0.00038389116525650024, 0.00046712160110473633, 0.00044670887291431427, 0.0004280582070350647, 0.0003967955708503723, 0.00038851797580718994, 0.00040996260941028595, 0.0003871005028486252, 0.0005486700683832169, 0.0005774344317615032, 0.0007018893957138062, 0.000561244785785675, 0.0007813796401023865, 0.0005500763654708862, 0.0005935542285442352, 0.0003592744469642639, 0.0005996581166982651, 0.0003949049860239029, 0.0005607940256595612, 0.0006201043725013733, 0.0004360117018222809, 0.0006516207940876484, 0.0003893908578902483, 0.0006382199935615063, 0.0006971657276153564, 0.0008493503555655479, 0.0007174843922257423, 0.0002918001264333725, 0.000798376277089119, 0.0005607903003692627, 0.0002856161445379257, 0.00021615531295537949, 0.0005365796387195587, 0.0005956860259175301, 0.0005809245631098747, 0.0008115656673908234, 0.0009082704782485962, 0.0012408941984176636, 0.0011927559971809387, 0.0012780055403709412, 0.0008913204073905945, 0.0009083077311515808, 0.0008430853486061096, 0.0002245083451271057, 0.0003996342420578003, 0.00022976507898420095, 0.00015343999257311225, 0.0002312681172043085, 0.0005767466500401497, 0.0007085129618644714, 0.0009390413761138916, 0.001082349568605423, 0.0012415647506713867, 0.0009628050029277802, 0.0007142666727304459, 0.0005869939923286438, 0.0007109418511390686, 0.000572890043258667, 0.000513024628162384, 0.0005948096513748169, 0.0006239227950572968, 0.0006890259683132172, 0.00046940892934799194, 0.0004077516496181488, 0.0008159857243299484, 0.0006092134863138199, 0.0005761804059147835, 0.0006487146019935608, 0.0007510259747505188, 0.0008931159973144531, 0.0008706115186214447, 0.0007128305733203888, 0.0004980489611625671, 0.000375114381313324, 0.0003950968384742737, 0.0004676207900047302, 0.00035019591450691223, 0.0002663061022758484, 0.0005382224917411804, 0.0005360543727874756, 0.00045875832438468933, 0.0006322339177131653, 0.0005574747920036316, 0.0007488913834095001, 0.0007071420550346375, 0.0004842206835746765, 0.0007164031267166138, 0.0006751567125320435, 0.0006937757134437561, 0.000687871128320694, 0.0007171817123889923, 0.0006598159670829773, 0.0005295276641845703, 0.000734865665435791, 0.0006674677133560181, 0.0005222372710704803, 0.000364743173122406, 0.00046123936772346497, 0.0003679264336824417, 0.0008360445499420166, 0.001253269612789154, 0.0010123848915100098, 0.0007870346307754517, 0.0007947832345962524, 0.0011133775115013123, 0.0013192445039749146, 0.001383662223815918, 0.0011230409145355225, 0.0009589493274688721, 0.0006461776793003082, 0.0004650671035051346, 0.0002446770668029785, 0.0006300527602434158, 0.0011080875992774963, 0.0014650523662567139, 0.0016261860728263855, 0.0013795346021652222, 0.0012196749448776245, 0.0009698718786239624, 0.001628294587135315, 0.001017741858959198, 0.0008906926959753036, 0.0008326712995767593, 0.0008570365607738495, 0.0008271224796772003, 0.0005358010530471802, 0.00042340531945228577, 0.0005153752863407135, 0.0009090974926948547, 0.0015838146209716797, 0.001078292727470398, 0.0010823607444763184, 0.0008956938982009888, 0.0007968749850988388, 0.0012508630752563477, 0.0012358129024505615, 0.0014269649982452393, 0.001010894775390625, 0.0007028579711914062, 0.0004273839294910431, 0.0003981795161962509, 0.0003166794776916504, 0.0005855411291122437, 0.001077413558959961, 0.000864565372467041, 0.001576066017150879, 0.0011492818593978882, 0.001144450157880783, 0.0009538382291793823, 0.0007348507642745972, 0.0010391771793365479, 0.0005941763520240784, 0.000560220330953598, 0.0007653422653675079, 0.0006037130951881409, 0.0006350651383399963, 0.0005493462085723877, 0.0008376538753509521, 0.0010010898113250732, 0.001434028148651123, 0.001158304512500763, 0.0008365660905838013, 0.0008697509765625, 0.0011471956968307495, 0.001145675778388977, 0.0012278854846954346, 0.0009372830390930176, 0.0011190176010131836, 0.0015645027160644531, 0.0015361607074737549, 0.001488327980041504, 0.0014733374118804932, 0.000973135232925415, 0.0016491413116455078, 0.0019164085388183594, 0.002523064613342285, 0.01538991928100586, 0.0094107985496521, 0.006039947271347046, 0.04696488380432129, 0.004027605056762695, 0.008596181869506836, 0.008463859558105469, 0.0646052360534668, 0.011429250240325928, 0.016497373580932617, 0.017302632331848145, 0.07097053527832031, 0.009931802749633789, 0.023543357849121094, 0.013293266296386719, 0.02494192123413086, 0.0401686429977417, 0.12970352172851562, 0.14960098266601562]         
z = np.max(f)
f = lst = [x - z for x in f]
f = [np.exp(x) for x in f]
print(f)
'''

all_attentions = get_attentions(tokens)
aver = aver_attn(all_attentions)
#draw_heatmap(all_attentions[0][11])
draw_heatmap(aver[0])
#print(len(aver[0][0]))
#draw_heatmap(aver[2])


#print(len(all_attentions))
#print(aver[0][0])
#print(len(all_attentions[0][0][0]))

        